Recurrent Neural Network

## Recurrent Neural Networks are commonly used in speech rocognition because it has an internal memory that keeps track of previous inputs adn temporal data, which all affects the subsequent prediction. In simpler words, every word in a sentence depends on each other, and when we are getting to the last words of a sentence, the result from the previous words still must be analyzed. It's important to note that traditional RNN models are very slow and training can be difficult, and more advanced models like LSTM are used for NLP. However, it's important to understand RNN first.

# Processing Categorical Features
## Starting off, we need to figure out the best technique for processing categorical features. Numerical features like age, year, or height should be left alone because their most important characteristic is that they can be compared, while binary features like head or tail can be easily replaced with 0 or 1. This leaves us with categorical values like country, and we might think that we can just create a hash table, where each country is assigned to an integer. However, if we have an example like ["China" = 1, "Japan" = 2, "India" = 3], a computer might logically process that India - Japan = China because 3 - 2 = 1, which isn't how the reality works, so that's why we must apply one-hot encoding. Therefore, China would be (1,0,0) in this case, while Japan and India are (0,1,0 and (0,0,1). And on a side note, even if single integers would work in this scenario, there isn't a way for us to process two countries at once, because having China and Japan at the same time requires putting two integers into one parameter. Alternativelu, representing that using one-hot encoding is as simple as (1,1,0).

# Processing Text Data
## Machines can only read numbers, which means we need to perform Word Embeddings. The first step is Tokenization, which is the prcoess of breaking down a text into a list of tokens. There are a couple of considerations during this process, including typos, stop words ("the", "a", "of", etc), and upper / lower case conversions. For example, "goooood" may be an verexaggerated version of "good" or "god" and not a typo, while "apple" is a fruit and "Apple" is a company. If there are too many tokens, the ones with the least frequencies should be removed to reduce dimensonal complexity and speed up computation. Tokenization may look easy, but there are a lot of work done during this process. The second step is to create a dictionary, where all the remaining tokens are assigned to an index. It's important to note that this dictionary only keeps track of all the unique words in the text, not how many times each appeared. With this, we can use sequence of indexes to represent the text.
<img width="1266" alt="Screen Shot 2022-07-04 at 10 52 31 PM" src="https://user-images.githubusercontent.com/102645083/177258684-df8a2a60-959a-4d89-8839-3da9a12f2aad.png">

# Text to Sequence
## The next step is to align the sequences of every text in a set to the same length, and we can do so by performing zero padding on short sequences or cutting out portions of long sequences. For example, if we have "This movie is great" and "I suggest not wasting your time watching this" and want the sequence length to be six, we can changed them to "null null this movie is great" and "not wasting your time watching this".
<img width="1282" alt="Screen Shot 2022-07-04 at 11 02 40 PM" src="https://user-images.githubusercontent.com/102645083/177260038-121a8dad-185d-4c21-9b81-e5f28b36c321.png">

## We then perform one-hot encoding on every word, which means if there are n amount of words, the one-hot vectors are n-dimensional.
<img width="1133" alt="Screen Shot 2022-07-05 at 12 25 24 AM" src="https://user-images.githubusercontent.com/102645083/177273002-18694bdd-3d00-49e4-9eec-958e7d975eb1.png">

## Since each word is represented by a ridiculously long matrix, we dimensionality reduce each word (from a 5000 x 1 to a 32 x 1 matrix in the movie review example).
<img width="750" alt="Screen Shot 2022-07-05 at 12 44 36 AM" src="https://user-images.githubusercontent.com/102645083/177276592-fdc18e50-0810-49ee-a205-a19d9de24c4f.png">

## With Word Embeddings out of the way, we can now look into RNN models. Like mentioned earlier, RNN is a good way to model sequential data, which means its neural network will look slightly different that CNN's. Instead of taking in every single word in the beginning, it only takes in the first word, performs embedding on it, and is represented by a dimension reduced matrix (32-D in this project). There are 32 initial weights that can be all zeros or random numbers in a range, and 32 neurons in each layer, which means each neuron takes in a a total of 64 inputs. They each then have one output, and the 32 outputs are now the new weights and are passed on to the next layer. The outputs of the final layer is then flattened into an equation and it goes through an activation function to become the result / prediction.
![Screen Shot 2022-07-10 at 3 10 21 PM](https://user-images.githubusercontent.com/102645083/178163841-018841e9-0a53-44bc-8d53-daa375d4a4a9.png)

## Back-Propogation for RNN is extremely complicated, and there are many separate topics associated with it. The first topic is Vanishing & Exploding Gradient, and here is an simple example: 1.01 to the power of 1,000 is around 20,959, while 0.99 to the power of 1,000 is around 0.00004. Because we know that finding the partial derivative in respect to a weight requires multiplying the partial derivatives of components that reside later in the network due to the chain rule, this product can be indefinitely long. We might never know what all these partial derivative values are equal to, so we might end up getting a near zero number or an indefinitely large value. Earlier weights are typically subjected to this problem, because the earlier it resides in the network, the more terms are going to be included in the product that calculates the gradient, and the more of these terms are less or more than one, the quicker the gradient is going to vanish or explode.

## Looking into the IMDB Movie Review project, the dataset has 50000 movie reviews that are labeled positive or negative, and half of them will be used for training and the other for testing.


